{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All code generated by Chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e0731600b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set seeds\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Q-Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise env\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "\n",
    "# Initialise models\n",
    "model = QNetwork(input_dim, output_dim).to(device)\n",
    "target_model = QNetwork(input_dim, output_dim).to(device)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "target_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "episodes = 500\n",
    "target_update_freq = 10\n",
    "replay_buffer = deque(maxlen=100_000)\n",
    "\n",
    "# Optimiser & Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters for DQN Training\n",
    "\n",
    "These hyperparameters guide how the DQN agent learns and explores:\n",
    "\n",
    "- **Gamma:** Determines how much future rewards matter (close to 1 means long-term rewards are valued highly).\n",
    "- **Epsilon:** Starting exploration rate (100% exploration at start).\n",
    "- **Epsilon Min:** The minimum exploration rate (ensures some exploration continues).\n",
    "- **Epsilon Decay:** Reduces epsilon over time, gradually shifting from exploration to exploitation.\n",
    "- **Batch Size:** Number of experiences sampled at once during training.\n",
    "- **Learning Rate:** How quickly the network updates its parameters.\n",
    "- **Episodes:** Number of episodes the agent trains for.\n",
    "- **Target Network Update Frequency:** How often the target Q-network gets updated.\n",
    "\n",
    "### Replay Buffer\n",
    "A replay buffer to stabilise training by reusing past experiences.\n",
    "\n",
    "### Optimiser & Loss Function\n",
    "- **Optimiser:** Adam optimiser, chosen for its stability and adaptive learning rate.\n",
    "- **Loss Function:** Mean Squared Error (MSE) loss, used to measure how accurately predicted Q-values match target Q-values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [1:33:58<00:00, 11.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for episode in trange(episodes):\n",
    "    state = env.reset()[0]\n",
    "    state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state.unsqueeze(0))\n",
    "                action = q_values.argmax().item()\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Training\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            minibatch = random.sample(replay_buffer, batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "            states = torch.stack(states)\n",
    "            next_states = torch.stack(next_states)\n",
    "            actions = torch.tensor(actions, dtype=torch.int64, device=device)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "            dones = torch.tensor(dones, dtype=torch.bool, device=device)\n",
    "\n",
    "            q_vals = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "            with torch.no_grad():\n",
    "                next_q_vals = target_model(next_states).max(1)[0]\n",
    "                target_q = rewards + gamma * next_q_vals * (~dones)\n",
    "\n",
    "            loss = loss_fn(q_vals, target_q)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Update target network\n",
    "    if episode % target_update_freq == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "print(\"Training finished.\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop for DQN (Simple Explanation)\n",
    "\n",
    "Here's how the agent learns over 500 episodes:\n",
    "\n",
    "- **Start an episode:**  \n",
    "  Reset the environment and get the initial state.\n",
    "\n",
    "- **Choose actions (Epsilon-greedy):**  \n",
    "  The agent either:\n",
    "  - Picks a random action (to explore), or\n",
    "  - Uses the trained model to pick the best-known action (to exploit).\n",
    "\n",
    "- **Interact with the environment:**  \n",
    "  Perform the chosen action, receive a new state and reward, and check if the episode is done.\n",
    "\n",
    "- **Store experience:**  \n",
    "  Save each experience (state, action, reward, next state, done) in memory.\n",
    "\n",
    "- **Train from experience:**  \n",
    "  Once there's enough experience:\n",
    "  - Randomly sample experiences.\n",
    "  - Calculate predicted and target Q-values.\n",
    "  - Update the network based on the difference (loss) between these values.\n",
    "\n",
    "- **Update Target Network:**  \n",
    "  Every 10 episodes, update the target model with the main model’s parameters.\n",
    "\n",
    "- **Reduce exploration gradually:**  \n",
    "  Decrease exploration over time, letting the agent focus more on learned actions.\n",
    "\n",
    "After training, the environment is closed, and the model is ready to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"lunarlander_dqn.pth\")\n",
    "print(\"model saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
